# The Moment Every AI Lab Dreaded Has Arrived!

## Video Information

- **Video ID:** `JtKf6dcT8hg`
- **URL:** https://www.youtube.com/watch?v=JtKf6dcT8hg&lc=UgwhZ8VjzmHuXskkpUd4AaABAg.AJiCWJJWiBSAJiJ_oXGDgn
- **Title:** The Moment Every AI Lab Dreaded Has Arrived!
- **Channel:** Pourya Kordi
- **Duration:** 16:19 (979 seconds)
- **Upload Date:** 20250623
- **View Count:** 35,565 views

## Transcript Metadata

- **Extraction Method:** yt-dlp
- **Language:** en
- **Line Count:** 867
- **Generated:** 2025-06-24 21:30:19

## Available Languages

N/A

## Plain Text Script

One of the largest AI models in the world was just deprecated for world was just deprecated for underperforming. This a nightmare for underperforming.

This a nightmare for AI labs. Many were banking on scaling AI labs.

Many were banking on scaling and emergent properties to finally make and emergent properties to finally make models more reliable and robust. models more reliable and robust.

Apparently, current training methods are Apparently, current training methods are failing to scale with more compute. On failing to scale with more compute.

On the other hand, Microsoft Research in collaboration with two Chinese companies introduce a new method of training that introduce a new method of training that is a compute hungry monster. It is a compute hungry monster.

It massively scales with more compute and it is already outperforming models double its size. Nvidia also introduced double its size.

Nvidia also introduced a new fascinating paper on the first AI a new fascinating paper on the first AI model that is open-minded and curious. model that is open-minded and curious.

This model is never satisfied with any solution and always seeks better understanding and better strategies. The researchers show that a curious AI model with an open mind can learn more general strategies because it doesn't settle down for any immediate solution.

And to top it all off, Isomeorphic Labs spun out of Google Deep Mind revealed some mind-blowing details about their project. They believe they can cure all diseases, claiming their AI is performing beyond our understanding.

This AI is so sophisticated that even PhD biologists just have to trust its judgment. I'm honestly so happy that you guys are into AI on such a deep technical and philosophical level.

I love talking about this and my channel is growing like a rocket ship. The number of positive comments and emails I get is literally insane.

I just wanted to thank you for the overwhelming to thank you for the overwhelming support and I'll do my best to keep publishing. Let's get into it.

Open AAI just announced that GPT 4.5 is just announced that GPT 4.5 is deprecated. GPT4.5 was one of the deprecated.

GPT4.5 was one of the largest AI models in the world and largest AI models in the world and apparently it showed no obvious improvement over the previous generation. Most people understandably don't realize the massive implications of this because they are seeing AI of this because they are seeing AI improving in other dimensions.

But here improving in other dimensions. But here is why this deprecation should be the is why this deprecation should be the number one headline right now.

AI has a number one headline right now. AI has a very unique property compared to other very unique property compared to other technologies.

If you want to send people to the space, you want the most efficient rocket. If you are designing a laptop, you want it to be as efficient as possible.

But when it comes to AI, you don't want an efficient AI. You want an AI model that is able to use the most amount of compute most efficiently.

It's like, do you prefer a lemonade stand like, do you prefer a lemonade stand that quadruples your $100 in a day or Google that returns 10% on a trillion dollar in a year? Sure, the lemonade dollar in a year?

Sure, the lemonade stand is more efficient, but the one stand is more efficient, but the one that is able to mobilize trillions of that is able to mobilize trillions of dollars in an effective and useful way dollars in an effective and useful way changes the world. The entire AI game is changes the world.

The entire AI game is make a system that can absorb enormous make a system that can absorb enormous amounts of compute in ways that unlock qualitatively new capabilities. Richard Sutton's bitter lesson is a must readad for anyone who wants to really get the context of the AI landscape.

Sutton context of the AI landscape. Sutton wrote this thing back in 2019 and it's wrote this thing back in 2019 and it's like he saw this coming.

He looked at 70 like he saw this coming. He looked at 70 years of AI research and noticed the same pattern happening over and over again.

It happened in speech recognition. The pattern is always the recognition.

The pattern is always the same. Researchers try to engineer a more same.

Researchers try to engineer a more intelligent model. And another approach intelligent model.

And another approach that is able to utilize more compute effectively just comes in and washes over every other approach using a bigger over every other approach using a bigger more compute inensive but qualitatively more compute inensive but qualitatively better model. So GPT4.5 wasn't just an better model.

So GPT4.5 wasn't just an enormous inefficient model that had to enormous inefficient model that had to retire. It was an attempt to show the current AI paradigm is able to absorb even more compute and possibly unlock even more compute and possibly unlock new emergent properties and that didn't new emergent properties and that didn't materialize.

This can only mean three materialize. This can only mean three things either data issue, training issue things either data issue, training issue or architecture issue.

And until they or architecture issue. And until they find and fix it, we are kind of stuck in the same realm.

Obviously, the reasoning models are showing promise over the base models and are able to utilize more compute. But they have a problem with generalization, meaning they are just getting better at very specific things, getting better at very specific things, which is totally useful, but unlikely to get us to artificial general intelligence.

That's why Microsoft intelligence. That's why Microsoft introduced a new training method that introduced a new training method that utilizes reinforcement learning in utilizes reinforcement learning in pre-training for a more general pre-training for a more general understanding.

But before we dive in, let's take a look at today's sponsor, UPDF. UPDF is an all-in-one PDF toolkit UPDF.

UPDF is an all-in-one PDF toolkit powered by AI. It's a simple tool with a powered by AI.

It's a simple tool with a modern user interface that handles your modern user interface that handles your PDF needs across all major platforms PDF needs across all major platforms from Mac OS and Windows to iOS and Android. UPDF turns your PDF files into editable documents.

The conversion is so editable documents. The conversion is so accurate and clean that you forget you accurate and clean that you forget you are working with what was originally an are working with what was originally an image.

Handling simple text in PDF is image. Handling simple text in PDF is easy, but take a look at this.

Here is a easy, but take a look at this. Here is a complicated diagram with lots of numbers complicated diagram with lots of numbers and elements.

I can move and edit this tiny number inside of another element and it works without a problem. You can of course edit all other elements like images, watermarks, links, footers, images, watermarks, links, footers, headers and basically anything you can headers and basically anything you can see inside the documents.

Plus, you can split or even merge multiple PDF files. As someone who reads AI papers every As someone who reads AI papers every day, this the part I appreciate the most.

an AI assistant that automatically understands the context of the paper and understands the context of the paper and is able to help me with even the most technical questions. Sure, I can upload the PDF into Google's Notebook LM, but I can't highlight a specific picture or diagram and ask about it.

With UPDF, I diagram and ask about it. With UPDF, I can see the actual document dark themed can see the actual document dark themed as well, right alongside the chat as well, right alongside the chat assistant.

And the UI, there isn't even assistant. And the UI, there isn't even a comparison.

So, if you interact with a comparison. So, if you interact with PDFs for school, work or personal PDFs for school, work or personal projects, you should check out UPDF projects, you should check out UPDF using the link in the description for an using the link in the description for an exclusive discount.

Microsoft in collaboration with two Chinese companies collaboration with two Chinese companies introduce reinforcement pre-training. It introduce reinforcement pre-training.

It offers a scalable method to leverage offers a scalable method to leverage vast amounts of text data for generalpurpose reinforcement learning rather than relying on domain specific annotated answers. Today's reasoning models have immediate and obvious models have immediate and obvious problems that everyone is trying to problems that everyone is trying to solve.

They are highly reliant on a specifically annotated data with verifiable solutions. So if for example verifiable solutions.

So if for example you want an AI that is great medical prescription, you need massive amounts of data specifically prepared for that of data specifically prepared for that purpose. The process of reinforcement purpose.

The process of reinforcement learning with verifiable rewards is learning with verifiable rewards is actually quite simple in terms of conceptual understanding. In reinforcement learning with human reinforcement learning with human feedback, an AI gives a response and a human picks the one that he prefers.

Now swap the human for an automated verifier swap the human for an automated verifier and you get reinforcement learning with verifiable rewards. This verifier lets you scale further and train on much more data.

But it needs deterministic criteria to make decisions. For example, criteria to make decisions.

For example, the answer must be a specific number for a math problem. The code should compile or the response should be true or false.

Basically any predetermined rule. The problem is the set of tasks with deterministic and easily verifiable answers is very small.

Also, preparing answers is very small. Also, preparing the data set takes a lot of manual effort.

Because of the nature of the problems and the difficulty of preparing the data set, the models end up being spiky and good at very specific things without developing general understanding. It's again Richard understanding.

It's again Richard Sutton's bitter lesson. Forcing Sutton's bitter lesson.

Forcing engineered verifiers with manual data engineered verifiers with manual data gives you marginal gains, but a more general approach with even less human involvement would likely sweep the board involvement would likely sweep the board once again. That's when Microsoft's new once again.

That's when Microsoft's new approach comes in. This might or might approach comes in.

This might or might not be the cure, but it shows how a clean sweep might look like. Here is the analogy they used.

Today's models are analogy they used. Today's models are trained like this.

Huge amounts of Nex trained like this. Huge amounts of Nex token prediction training, also known as token prediction training, also known as pre-training, on basically the entire pre-training, on basically the entire internet's text data.

and then reinforcement learning as the cherry on top. The important distinction being pre-training doesn't require annotations.

The model is just predicting a word and it is rewarded when it is correct. So now instead of adding the reinforcement as the cherry on top, how about making a cherry cake, meaning adding the reinforcement to the token prediction process.

In this token prediction process. In this approach, in training, the model doesn't approach, in training, the model doesn't just predict the next token.

It first just predict the next token. It first gives you a reason why it thinks the next token should be a certain word.

Then it submits prediction. The Then it submits prediction.

The training doesn't care about the training doesn't care about the reasoning steps. So the model is free to think however it wants.

But it is rewarded only if the final token is correct. The model is reinforced to think from the ground up and it can learn to generally reason using any corpus of data without the specific corpus of data without the specific annotations.

As you see, this approach annotations. As you see, this approach seems like a huge waste of compute seems like a huge waste of compute trying to produce multiple chains of trying to produce multiple chains of thoughts for each token prediction.

Plus thoughts for each token prediction. Plus token prediction is not really a token prediction is not really a sophisticated reasoning task.

So most of sophisticated reasoning task. So most of the compute on thoughts is just wasted.

But it again proves the potential of more scalable solutions even when it's inefficient. The results of this approach by incentivizing the capability of next token reasoning.

RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong tokens.

Moreover, RPT provides a strong pre-trained foundation for further pre-trained foundation for further reinforcement fine-tuning. The scaling reinforcement fine-tuning.

The scaling curves show that increased training compute consistently improves the next token prediction accuracy. In next token prediction, the RPT model almost matches Quen 32B which is more than twice its size and it outperforms the Quen 14B reasoning and Quen 32B base model in both MMLU and Super GPQA.

In the same spirit, Nvidia showed a different method spirit, Nvidia showed a different method of RL called Pro RL in their analysis. of RL called Pro RL in their analysis.

One other reason AI models don't show general improvement through reinforcement learning is that they get stuck on one solution and they sort of lose curiosity. This a very lose curiosity.

This a very interesting work that shows even cutting interesting work that shows even cutting edge models can unlock a new level of reasoning and generalization with very minor adjustments. ProL stands for minor adjustments.

ProL stands for prolonged reinforcement learning. The prolonged reinforcement learning.

The researchers basically incentivize the researchers basically incentivize the model to never settle for a strategy by always rewarding a bit of exploration even after the model has settled for an almost perfect solution. This new approach incentivize the model to always approach incentivize the model to always look for better and more general look for better and more general solutions not being stuck in a local maxima.

It gets better in the areas that it is already good at, but it shows even more improvement in the areas that it was failing at. showing when the model was failing at.

showing when the model is forced or incentivized to not settle is forced or incentivized to not settle down too soon, it finds more general down too soon, it finds more general patterns that it can apply across patterns that it can apply across multiple tasks. This seems like a very multiple tasks.

This seems like a very simple idea, but the innovation is simple idea, but the innovation is actually making it work, which is a bit too technical for this video, but we can take a look at the results. The take a look at the results.

The benchmarks show very concrete and benchmarks show very concrete and significant improvements over the base significant improvements over the base model in almost all domains. It even consistently outperforms Distill Quen 7B, which is about five times bigger than the base model.

This one of the most immediately applicable papers I've most immediately applicable papers I've read in recent months with very concrete results. But probably the most exciting thing I heard this week was Isomorphic thing I heard this week was Isomorphic Labs conviction about curing all Labs conviction about curing all diseases.

Can we just appreciate the diseases. Can we just appreciate the fact that they can even hope for it?

This claim that AI is going to be able to solve all diseases. Is that realistic?

Max, this isn't going to happen overnight. Let's be clear.

But I think the exciting thing is that we can think the exciting thing is that we can actually see there's perhaps a practical actually see there's perhaps a practical path towards that point. And it's very path towards that point.

And it's very different at this point in time than it different at this point in time than it has ever been because we've got these AI machine learning models that understand the biological world and the biochemical the biological world and the biochemical world in a completely different manner than what we've had before.

And so that's opening up, you know, tons of that's opening up, you know, tons of disease space that we didn't think was tractable before. Uh, and that's just today.

So as we start to develop these models further and further, it's really just the very beginning. Does that mean just the very beginning.

Does that mean I mean every disease is on the table here, Becky? So I would say nothing is off the table at this point and you know the journey for me here is you know been a big one.

I used to be much more a big one. I used to be much more conservative in this space but having conservative in this space but having come to isomeorphic labs seeing you know come to isomeorphic labs seeing you know the kind of models that we have things that I thought in the past we would never be able to predict and now being never be able to predict and now being able to do it every day within you know able to do it every day within you know 5 10 seconds um completely shifted my 5 10 seconds um completely shifted my mindset so now I would put nothing off mindset so now I would put nothing off the table they sound so confident that the table they sound so confident that even if they achieve 30% of what they even if they achieve 30% of what they are hoping for they're going to change are hoping for they're going to change the world literally nothing would raise the world literally nothing would raise the standard of living more than this the standard of living more than this besides people not waging wars one Of the most interesting parts of the interview was biologists saying they interview was biologists saying they don't really understand the AI but they have to trust its instinct because the AI understands biology so much better.

AI understands biology so much better. If you've ever wondered what super If you've ever wondered what super intelligence looks like, here it is.

intelligence looks like, here it is. These models and you know can start These models and you know can start finding completely novel chemical matter for completely novel you know mechanisms that no one's really discovered that no one's really discovered before which is mind-blowing for me as a computer scientist.

Yeah, that's mind-blowing for me. mind-blowing for me.

There's been some like um almost like career defining moments like where you the AI will or the will give you a hypothesis right it will suggest something and you think like I'm not something and you think like I'm not convinced I would do that but the convinced I would do that but the model's telling me this thing and it's really quite convinced about this thing so I should maybe just test this so I should maybe just test this hypothesis and then actually it turns hypothesis and then actually it turns out that the model was right um and you were absolutely right to test it and it's really pushed forward your it's really pushed forward your project or even like that kind of field.

So, um I think for me it's not about how we trust the models, it's about how we trust the models, it's about how we are open to testing the hypotheses that are open to testing the hypotheses that they put in front of us. Um and not sort of going, oh that doesn't fit with my worldview, so I'm not going to test my worldview, so I'm not going to test it.

But then you are also human, right? So I do wonder whether if you see, you So I do wonder whether if you see, you know, lots of hits with the model as it were.

If the model is coming up with lots of good stuff in a row, do you start sort of maybe trusting it more than yourself? We actually put a lot of than yourself?

We actually put a lot of trust in the models. We actually use for example some of the models we have we use them as quite strict cutoffs.

What kind of cutoff? Like for example we have a model which we call binding a model which we call binding probability and it goes from zero to one.

So one is um you know the model is convinced your model your molecule is convinced your model your molecule is definitely going to bind to your this definitely not going to bind and you know you can build a little bit of you know you can build a little bit of confidence over time that the model confidence over time that the model really does understand okay anything really does understand okay anything below 7 is really got a very low probability of success.

So we just defi define that as a cutoff and be like define that as a cutoff and be like we're not going to put anything in the we're not going to put anything in the lab that's got a probability of less than this because actually the model's quite likely to be right. It's probably quite likely to be right.

It's probably not going to be any good. And so you do even though and that's quite hard because as a chemist you design something and you think that was a really clever idea that I just came up with and like why doesn't it like it AI has some understanding of the AI has some understanding of the biological word that can't even begin to put in words.

The patterns it understands are beyond explanation and understands are beyond explanation and comprehension for humans. The difference comprehension for humans.

The difference is when people talk about super is when people talk about super intelligence, they normally mean it has intelligence, they normally mean it has the same superior understanding in the same superior understanding in everything, not just biology. But this everything, not just biology.

But this example helps us imagine how would a example helps us imagine how would a superior intelligence probably look like superior intelligence probably look like to us and how it is very unlikely that humans are the ceiling of intelligence. At this point, we will have AI models that are so much more intelligent or understand patterns so much better that understand patterns so much better that we just have to trust their judgment on everything.

That's why we have to make sure this thing has our best interest in sure this thing has our best interest in mind right now before reaching that mind right now before reaching that point. I'm really glad that AI labs are going back to trying out really novel approaches.

It makes up for very good approaches. It makes up for very good content, but I think very exciting content, but I think very exciting improvements and advancements are just improvements and advancements are just around the corner.

If you don't want to miss it, like and subscribe. Thanks for watching.


## Quality Analysis

- **Total Lines:** 867
- **Unique Lines:** 867
- **Duplicate Lines:** 0
- **Quality Score:** 100.0%

### âœ… High Quality Transcript

No duplicate lines detected.



## MCP Resource Usage

This transcript can be used as an MCP resource:

### Resource URI
```
transcript://JtKf6dcT8hg
```

### Programmatic Access
```python
# In your MCP server
async def get_transcript(video_id: str):
    return await load_transcript_resource(video_id)
```

### Use Cases
- **Content Analysis:** Analyze themes, topics, and sentiment
- **Quote Extraction:** Find specific quotes or statements  
- **Study Notes:** Generate structured educational notes
- **Search & Discovery:** Full-text search within video content
- **Summarization:** Create abstracts and key points
- **Fact Checking:** Verify claims and statements

---

*Generated by YouTube to MCP Resource Tool v1.0*  
*For more information: https://github.com/your-repo/mcp-youtube-transcript*
