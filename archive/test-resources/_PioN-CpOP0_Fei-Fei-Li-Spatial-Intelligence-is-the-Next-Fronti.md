# Fei-Fei Li: Spatial Intelligence is the Next Frontier in AI

## Video Information

- **Video ID:** `_PioN-CpOP0`
- **URL:** https://www.youtube.com/watch?v=_PioN-CpOP0&pp=0gcJCcEJAYcqIYzv
- **Title:** Fei-Fei Li: Spatial Intelligence is the Next Frontier in AI
- **Channel:** Y Combinator
- **Duration:** 44:22 (2662 seconds)
- **Upload Date:** 20250701
- **View Count:** 50,292 views

## Transcript Metadata

- **Extraction Method:** yt-dlp
- **Language:** en
- **Line Count:** 921
- **Generated:** 2025-07-02 23:10:45

## Available Languages

N/A

## Plain Text Script

My entire career is going after problems that are just so hard, bordering, that are just so hard, bordering, delusional. To me, AGI will not be complete without spatial intelligence.

And I want to solve that problem. I just love being an entrepreneur.

Forget about what you have done in the past. Forget about what others think of you.

Just hunker down and build. That is my [Music] So, I'm super excited here to have Dr.

I'm sure a lot of you know her, I know you too. Um, she's been she's been uh named the Um, she's been she's been uh named the godmother of AI.

One of the first godmother of AI. One of the first projects that you created was ImageNet projects that you created was ImageNet in 2009, 16 years ago.

That has over 80,000 citations and it really kicked off one of the legs of stools for AI off one of the legs of stools for AI which is the data problem. Tell us about which is the data problem.

Tell us about how that project came about. It was how that project came about.

It was pretty pioneering work back then. pretty pioneering work back then.

Yeah. Well, first of all, Diana and Gary and everybody, thanks for inviting me here.

Um, I'm so excited to be here. Um, I'm so excited to be here because I feel like I'm just one of you.

I'm also a entrepreneur right now. I just started a small company.

So, very just started a small company. So, very excited to be here.

Image then was um yeah, you're right. We actually conceived that uh almost 18 years ago.

conceived that uh almost 18 years ago. It's time really flies.

Uh, I was a first year assistant professor at Princeton. Princeton.

And uh and um the world of AI and machine learning was so different at that time. There was very little data.

Algorithms at least in computer vision did not work. There was no industry.

You know, as far as the public was concerned, the word AI doesn't exist. concerned, the word AI doesn't exist.

But there is a a still a group of us starting from the founding fathers of AI, right? John McCarthy and then we go AI, right?

John McCarthy and then we go through people like Jeff Hinton. I think through people like Jeff Hinton.

I think we just had a AI dream. We really want to make machines to think and to work.

And with that dream I was my own personal dream was to make machines see because seeing is such a cornerstone of intelligence. Visual intelligence is not just perceiving it's really just perceiving it's really understanding the world and do things in the world.

So I was obsessed with the problem of making machines see and as I was obsessive obsessively developing machine learning algorithms at that time machine learning algorithms at that time we did try neuronet network but it we did try neuronet network but it didn't work. We pivoted to base net to support vector machines whatever it was.

But one problem always haunted me and it was the problem of generalization. If you're working in machine learning If you're working in machine learning you have to respect that generalization you have to respect that generalization is the core mathematical foundation or is the core mathematical foundation or goal of machine learning.

And in or goal of machine learning. And in order to generalize um these algorithms order to generalize um these algorithms needs data.

Yet no one had data at that needs data. Yet no one had data at that time in computer vision and I was the time in computer vision and I was the first generation of grad student who was first generation of grad student who was starting to dabble into data because I was the first generation of graduate student who saw the internet the big internet of things.

So fast forward um 2000 around 2007ish 2000 around 2007ish my student and I decided that we have to take a bold bet. We have to bet that there needs to be a paradigm shift in machine learning and that paradigm shift has to be led by datadriven methods and there was no data.

So we're like okay let's go to the internet download a let's go to the internet download a billion images that's the highest number billion images that's the highest number we could get on the internet and then just create the world's the entire world's uh visual taxonomy and uh we use that to train and benchmark machine learning algorithm and that was why imageet was conceived and uh came to life and it took a while until there were algorithms that were promising It wasn't until 2012 when Alex net came out and that was the second part of the equation with getting to AI was getting the compute and throwing enough at it and algorithms.

Tell us about what was and algorithms. Tell us about what was that moment where you started to see oh that moment where you started to see oh you seated it with data and now people started the community started to figure more things out for AI.

Right. So between 2009 we published this tiny little CVPR poster.

Um um in 2009 to little CVPR poster. Um um in 2009 to 2012 the Alex that there were three years that we really believe that data will drive AI but we had very little signal in terms of if that was working.

So we did a couple of things. One is we open sourced.

I we believed One is we open sourced. I we believed from the get-go we have to open source this to the entire research community for everybody to work on this.

The other thing we did is we created a challenge because we want the whole world's uh because we want the whole world's uh smartest uh students and researchers to smartest uh students and researchers to work on this problem. So that was what we call the image that challenge.

So every year we release a uh a testing data set. Well, the whole image that is there for training, but we release there for training, but we release testing and then we invite everybody testing and then we invite everybody openly to participate and then the first couple of years was really setting the baseline.

You know, the performance baseline. You know, the performance was in the 30% error rate.

It wasn't zero or I mean it wasn't completely random, but it wasn't that great. But um the third year 2012 I you know I wrote this in a book that I published but I this in a book that I published but I still remember it was um it was around still remember it was um it was around the end of summer that we were we were taking all the results of image that challenge and running it on our that challenge and running it on our servers.

And I remember it was late servers. And I remember it was late night one day I got a ping from my graduate student.

I was home um and said we got a result that really stand out and you should take a look and we we looked into it. It was convolutional neuronet network something.

It wasn't called Alex that that time that team that Jeff Hinton's team was called supervision. It was a very clever play supervision.

It was a very clever play of the word super as well as supervised of the word super as well as supervised learning. So supervision and we look at learning.

So supervision and we look at what supervision did. It was an old algorithm.

Convolutional neuronet network was published in the 1980s. Um there was a couple of tweaks in terms of the algorithm but it was pretty the algorithm but it was pretty surprising at the beginning for us to see that there was such a step change.

And of course we you know uh we I mean the rest of the history you all know we presented this in the image that presented this in the image that challenge workshop in uh that year's ICCV Florence Italy and uh Alex Kushvski came and many people came. I remember young lak also came and um and now the world lak also came and um and now the world knows this moment as um the image that challenge Alex that moment um I do want to say that the it's not just uh convolutional neuronet network it was also the first time that two GPUs were put together by Alex and his team and were used for the computing of and were used for the computing of deep learning.

So it was really the first moment of data GPUs and uh neuronet network coming together. Now neuronet network coming together.

Now following this trend of the arc of following this trend of the arc of intelligence for computer vision, intelligence for computer vision, imagenet was really the seat to solve the concept of object recognition. Then right after that it started to also AI got to the point that could solve the scenes right because you had a lot of scenes right because you had a lot of the work with your students like Andrew Kaparthi being able to describe a scenes.

Tell us about that transition from objects to a scenes. Yeah.

So image that was solving the problem of you present you're presented with an image and then you call out objects. There's a cat there's a chair and all that.

That's a fundamental problem in visual a fundamental problem in visual recognition. But ever since I was a graduate student entering the field of AI, I had a dream.

I thought it was a AI, I had a dream. I thought it was a hundred year 100y year dream which is hundred year 100y year dream which is storytelling of the world is that when storytelling of the world is that when humans open their eyes, you know, humans open their eyes, you know, imagine you just open your eye in this room.

You don't just see person chair chair. You actually see a um conference room, you know, with screen, with stage, with people, with the, you know, the crowd, the cameras, you actually can describe the entire scene.

actually can describe the entire scene. And that's a human ability that is at the foundation of uh visual intelligence and it's so critical for us to use in terms of our everyday life.

So I really thought that problem will take my entire life. I I literally when I graduated as a graduate student, I told myself on my deathbed, if I can create an algorithm that can tell the story of a scene, I've succeeded.

That was how I thought my career would be. IC Alex that thought my career would be.

IC Alex that moment came um deep learning took off moment came um deep learning took off and then when Andre and then later and then when Andre and then later Justin Johnson entered my lab we start Justin Johnson entered my lab we start to see signals of um natural language you know and visions start to collide and then Andreas and I proposed collide and then Andreas and I proposed this problem of captioning images or storytelling and long story short 2015 around 2015, Andre's uh Andrea and I published a series of papers that was among the first with a couple of concurrent papers of making literally a computer that of making literally a computer that captioned an image.

It was I almost captioned an image. It was I almost felt like what am I going to do with my life?

That was my lifelong go, you know? It was such an incredible moment for for both of us.

Um and um you know last year I gave a TED talk and I know last year I gave a TED talk and I actually used a uh something that Andre actually used a uh something that Andre tweeted a couple of years ago around the tweeted a couple of years ago around the time he finished image captioning work. time he finished image captioning work.

That was pretty much his dissertation. That was pretty much his dissertation.

I actually joked with him. I said, "Hey, I actually joked with him.

I said, "Hey, Andre, why don't we do the reverse? Take Andre, why don't we do the reverse?

Take a sentence and generate an image." And a sentence and generate an image." And of course he knew I was joking and he said, "Haha, I'm out of here." The the world was just not ready. But now fast world was just not ready.

But now fast forward, now we all know generative AI, forward, now we all know generative AI, you know, now we can take a sentence and generate beautiful pictures. So, so this moral of the story is um AI has seen an incredible growth and personally I feel I'm the luckiest person in the world I'm the luckiest person in the world because my entire career started at the because my entire career started at the very beginning of the end of AI winter the beginning of AI starting to you know take off and so much part of my own work my own career is part of this change or helped with this change.

So I feel so helped with this change. So I feel so fortunate and lucky and in a way fortunate and lucky and in a way proud and I think the wildest thing even to achieve your lifelong dream of describing a scenes and even generating describing a scenes and even generating them with diffusion models you're them with diffusion models you're actually dreaming bigger because the actually dreaming bigger because the whole arc of computer vision went from whole arc of computer vision went from objects to a scenes and now this concept of world and you actually decided to move from academia being a professor to now being the founder and CEO of World Labs.

Tell us about what world is. It's even harder than is scenes and objects.

even harder than is scenes and objects. Yeah, it is.

It is kind of wild. Um, so of course you all know the past it's really hard to summarize the past five or six years.

It it for me it's we're or six years. It it for me it's we're living in such a civilizational moment living in such a civilizational moment of this technologies uh progress, right?

While computer vision as a computer vision scientist we're seeing this vision scientist we're seeing this incredible growth you know from image incredible growth you know from image that to image captioning to image uh that to image captioning to image uh generation using some of the diffusion generation using some of the diffusion techniques.

While this happening in a techniques. While this happening in a very exciting way, we also have another extremely exciting thread which is language which is LLMs which is that really 2022 November Chad GBT blasted open the door of truly working open the door of truly working generation models that can basically generation models that can basically pass the touring test and all that.

So, so that this becomes very inspirational even for someone as old as me is to really think audaciously about what's next. And um I have a habit as a computer vision scientist.

A lot of my inspiration actually come from evolution inspiration actually come from evolution as well as brain science. I I find as well as brain science.

I I find myself in many moments of my career myself in many moments of my career where I'm looking for the next north where I'm looking for the next north star problem to solve. I ask the I ask star problem to solve.

I ask the I ask myself what is what evolution has done myself what is what evolution has done or what brain development has done. And there's something that's really important to notice or to appreciate.

important to notice or to appreciate. The development of human language in The development of human language in evolution took about, if you're super evolution took about, if you're super generous, let's just say it took about generous, let's just say it took about 300 to 500 million years, less than a million years.

That's the that's the length of evolution that took to develop a human language. And pretty much humans are the only animals that has sophisticated language.

We could argue about animal language but really about animal language but really language in its totality in terms of language in its totality in terms of being a tool of communication, being a tool of communication, reasoning, abstraction, it's really reasoning, abstraction, it's really humans. So that took less than even half a million years.

But think about vision. Think about the capability of understanding 3D world, communicate the 3D world.

That journey took evolution 540 million years. took evolution 540 million years.

The first trilabitete developed a sense of vision underwater 540 million years ago. And since then really vision was ago.

And since then really vision was the reason that set off this the reason that set off this evolutionary arms race. Before vision, evolutionary arms race.

Before vision, animals were simple for you know the half billion years before vision there's just simple animals. But the next half billion years 540 million years because of the capability of seeing the world understanding the world evolutionary arms race began and evolutionary arms race began and animal intelligence just start to to animal intelligence just start to to race each other.

So for me solving the problem of spatial intelligence to understand the 3D world to generate the 3D world to reason about the 3D world to do things in the 3D world is a fundamental problem of AI. To me AGI will not be complete without spatial will not be complete without spatial intelligence and I want to solve that problem and uh and that involves creating world models.

World models that creating world models. World models that goes beyond flat pixels.

world models that goes beyond language. World model that truly capture the 3D structure and the spatial intelligence of the world.

And the luckiest thing in my life is no matter how old I am, I always get to work with the best young people. So I um work with the best young people.

So I um you know I founded a c company with three incredible young but worldclass technologist Justin Johnson, Ben Mildenhal and Kristoff Lassner and we are just going to try to solve in my opinion the hardest problem um in AI right now which is um in AI right now which is incredible talent. I mean Chris, he was the creator of Pulsar which was the initial seed before Gosh and Splats that do a lot of differentiable rendering.

There's Justin Johnson, your former There's Justin Johnson, your former student who really has this super system student who really has this super system engineering mind that got real time engineering mind that got real time neural style transfer. Then you got Ben neural style transfer.

Then you got Ben who was the author of Nerf paper. So who was the author of Nerf paper.

So this a super crack team and you need such a crack team because we were chatting a bit about that vision is chatting a bit about that vision is actually harder than LLM to some extent. Maybe this a controversial uh uh say thing to say because LLMs are say thing to say because LLMs are basically 1D, right?

But you're talking basically 1D, right? But you're talking about understanding a lot of the 3D about understanding a lot of the 3D structures.

Why is this so hard and it's structures. Why is this so hard and it's behind language research?

you you really appreciate that. you you empathize how hard our problem is.

Syllabus comes in sequence. I mean, this why sequence to sequence, sequence modeling is so classic.

There's modeling is so classic. There's something else that is language that people don't appreciate.

There's no language in nature. You don't touch language.

You don't see language. Language literally comes out of Language literally comes out of everybody's head and that's a purely generative signal.

Of course, you put it on a piece of paper, it's there. But but the generation, the construction, the utility of language is very generative.

The world is far more complex than that. First of all, the real world is 3D.

And if you add time, it's 4D. But just let's just confine it's 4D.

But just let's just confine oursel within space. It's fundamentally oursel within space.

It's fundamentally 3D. So that by itself is a much more combinatorily harder problem.

Second, the sensing the the the sensing the the the reception of the visual world is a um is a projection. Whether it's your eye, your retina or a camera, it's always collapsing 3D to 2D.

And you have to appreciate how hard it is. It's mathematically illposted.

So you have to this why humans and animals have multi- sensors. Um and then you have to solve that problem.

And um and third, the world is not purely generative. Yes, we could generate generative.

Yes, we could generate virtual 3D world. It still has to obey physics and all that.

But there is also a real world out there. You are now subtly dialing between generation and reconstruction in a very fluid way.

And the user behavior, the utility, the use cases are very different. If you dial all the way to uh generation, we can talk about gaming and metaverse and all that.

If you dial all the way to real world, you're you we're talking about uh robotics and all that. But all this on a continuum of world modeling and spatial intelligence.

So it's a and of course the elephant in the room is there's a lot of data on the internet for language and where is the data for language and where is the data for spatial intelligence you know it's all spatial intelligence you know it's all in our head of course but it's not as easily as accessible as language.

So these are the reason it's so hard but frankly it excites me because if it's easy somebody else has solved it and my entire career is going after problems entire career is going after problems that are just so hard bordering that are just so hard bordering delusional and I think this the this this the delusional problem.

Thank you for supporting that. And even thinking about this from first principles, the human brain has a lot principles, the human brain has a lot more in the visual cortex and amount of neurons that process visual data as opposed to language.

How does that opposed to language. How does that translate into the model architectures translate into the model architectures are very different from LLMs from what are very different from LLMs from what you're kind of finding out, right?

Yeah, you're kind of finding out, right? Yeah, that's actually a really good question.

that's actually a really good question. And I mean there's still different And I mean there's still different schools of thoughts out there, right?

schools of thoughts out there, right? There is the LLM a lot of what we see in LLM is really writing the writing scaling law all the way to happy ending and you can almost you can just brute force self supervision all the way.

Um constructive world model might be a little more nuanced. The world is more structured.

there might be signals that we need to there might be signals that we need to use to guide it. You can call it in a shape of prior, you can call it supervision in your data, whatever it supervision in your data, whatever it is.

I think that these are the some of is. I think that these are the some of the open uh questions that we have to solve.

But but you're you're you're right and also if you think about human first of all, we don't have all the answers even to human perception, right? answers even to human perception, right?

How does 3D work in human vision is not a solved problem. We know mechanically the two eyes had to triangulate the two eyes had to triangulate information but even after that where is the mathematical model and we're not that great humans are not that great as that great humans are not that great as 3D animals.

So so um there is a lot that that is to be to be answered. So we are definitely at World Lab.

I'm just counting on really counting on one thing. I'm counting on we have the smartest people in the pixel world to solve this.

Is in the pixel world to solve this. Is it fair to say that what you're building at World Labs is these whole new foundation models where the output are 3D worlds?

And what are some of the applications that you're envisioning? because I think you listed you listed everything from you listed you listed everything from perception to generation.

So there's perception to generation. So there's this always this uh tension between uh this always this uh tension between uh generative models and discriminate generative models and discriminate models.

So where what we what models. So where what we what these 3D worlds do?

Yeah. So I'm not going to be able to talk too much about the details of world labs per se, but in terms of spatial intelligence, that's what it also excites me.

Just like what it also excites me. Just like language, the use case is so huge from language, the use case is so huge from creation which you can think about um designers, architects, industrial uh industrial designers as well as just artists, 3D artists, gave game developers from creation all the way to robotics, robotic learning, the utility of spatial intelligence model or world models is really big.

So um and then there are many um many related industries from marketing to entertainment to even metaverse. I'm actually really excited by actually really excited by metaverse.

I know so many people are kind of still like uh it's still not working. I know it's still not working.

That's why I'm excited because I think That's why I'm excited because I think the convergence of hardware and software the convergence of hardware and software will be coming. So that's that's also will be coming.

So that's that's also another great use case down the road. another great use case down the road.

I'm personally very excited that you're I'm personally very excited that you're solving metaverse. I gave it a try in my previous company.

So I'm so excited that you're doing that now. Yeah.

Well, I think I think there's more signal. I mean, I do think hardware is part of the part of the hurdle, but you know, you need content creation and metaverse content creation needs world models.

Let's switch gears a little bit. So Let's switch gears a little bit.

So maybe to some of the audience they might find your transition from going from academia to now being a founder CEO to academia to now being a founder CEO to be sudden but you actually have the be sudden but you actually have the remarkable journey through your whole remarkable journey through your whole life.

This not your first time you gone zero to one. You were telling me about how you immigrated to the US and about how you immigrated to the US and you didn't speak any English in your you didn't speak any English in your teens and you even ran a laundromat for teens and you even ran a laundromat for a good number of years.

Tell us about a good number of years. Tell us about how all those skills shaped who you are now.

Right. I'm sure you guys are here trying to listen to how to start a laundry mat.

Um yeah, that was when you were 19, right? Yeah, I was 19 and that was out of desperation.

So, um I had no means of supporting my family, my parents, and I need to go to college to be a physics major at Princeton. So I started a dry cleaning shop and in Silicon Valley language I fundraised.

I was the founder CEO. I was also the cashier and all the other things and I exited.

So after seven years [Music] all right you guys are very kind. I've never got claps for my laundry mat but thank you.

So, but anyway, I think thank you. So, but anyway, I think Diana's point, especially to all of you, I look at you.

I'm so excited for you because you're like literally half my because you're like literally half my age or even, you know, maybe 30% of my age and you're so talented. Just do it.

Don't be afraid. You know all my entire career of course I did laundry entire career of course I did laundry mat but uh even as a professor I chose couple of times I chose to go to departments where I was the first departments where I was the first computer vision professor and I that was against a lot of advice you know as a young professor you should go to a place where there's a community and senior mentors of course I would love to have senior mentors but if they're not there I still have to blaze my trailblaze my I still have to blaze my trailblaze my way right So I wasn't afraid of that.

And then I did go to Google to learn a lot about business in Google cloud and B2B and all those. And then I started a startup within Stanford started a startup within Stanford because around 2018 AI was not only taking over the industry a AI became a human a human problem.

Humanity will human a human problem. Humanity will always advance our technology but we always advance our technology but we cannot lose our humanity.

And I really cannot lose our humanity. And I really care about creating a beacon of light in the in the progress of AI and try to imagine how AI can be human- centered, how we can create AI to help humanity.

So I went back to Stanford and created human center AI institute and ran that as a startup for five years. um as a startup for five years.

um probably some people were not too happy I ran it as a startup for five years at in a university but I was very proud in a university but I was very proud of that. So in a way um I think I just love being an entrepreneur.

I love the feeling of ground zero like standing on feeling of ground zero like standing on ground zero. Forget about what you have done in the past.

Forget about what others think of you. just hunker down others think of you.

just hunker down and build. That is my comfort zone and I and build.

That is my comfort zone and I just love that. The other really cool just love that.

The other really cool thing about you, another in on top of thing about you, another in on top of all the awesome things you've done, you advise a lot of legendary researchers like Andre Kaparthy, Jim Fan who's at like Andre Kaparthy, Jim Fan who's at Nvidia, Jad Deng who's your uh co-author Nvidia, Jad Deng who's your uh co-author for ImageNet, they all went on to have these incredible careers.

what really stood out about them when they were stood out about them when they were students like advice for the audience students like advice for the audience that you could tell ah this person is gonna change the field of AI and you could tell so first of all I'm the lucky could tell so first of all I'm the lucky one I don't I think I owe more to my one I don't I think I owe more to my students than the other way around they students than the other way around they really make me a better person better teacher better researcher and having worked with so many like you said legendary students um is really the the the the the the honor of my life.

So, they're very different. Some of them are just pure scientists trying to hunker down and solve a scientific problem.

Some of them are industrial leaders. Some of them are industrial leaders.

Some of them are, you know, the greatest, you are, you know, the greatest, you know, disseminator of uh AI knowledge. know, disseminator of uh AI knowledge.

But I think there is one thing that But I think there is one thing that unifies them and I would encourage every single one of them of you to think about this. I also for those founders who are hiring this also my hiring criteria is I look for intellectual fearlessness.

is I look for intellectual fearlessness. I think it doesn't matter what where you come from.

It doesn't matter what problem we're trying to solve. That problem we're trying to solve.

That courage, that fearlessness of embracing courage, that fearlessness of embracing something hard and go about it and and be all in and trying to solve that in however way you want is really a core characteristic of people who succeed. I learned this from them and I really look for young people who have that and then that as a CEO at World Labs in my hiring I look for that quality.

So you're hiring a lot for World Labs too. So you're looking for that same trait, right?

Yes. Um I get permission from Diana to say that we're hiring.

We are hiring engineering talents. We're hiring product talents.

We're hiring 3D talents. We're hiring generative uh model um uh talents.

So, so I if you feel you're fearless and you're passionate about solving spatial intelligence, talk to me or come to our intelligence, talk to me or come to our website. Cool.

We're going to open it up for questions for the next 10 minutes. Hi Fay, thank you for your talk.

I'm a big big fan and um yeah so my question is more than two yeah so my question is more than two decades ago you worked on visual decades ago you worked on visual recognition I am I want to start my PhD what should I work on so I become a legend like you are I want to give you a thoughtful answer because I can always say do whatever excites you so first of all I think AI research has changed because um because academia if you're because um because academia if you're starting a PhD you are in academia.

Academia no longer has most of the AI resources. It's very different from my resources.

It's very different from my time, right? The chip uh the compute and the data are are kind of are really low in terms of resourcing academia and then there are problems that industry can run a lot faster.

So that industry can run a lot faster. So as a PhD student, I would recommend you as a PhD student, I would recommend you to look for those north stars that are to look for those north stars that are not on a collision course of problems that industry can solve better using better compute, better data and team better compute, better data and team science.

But there are some really science. But there are some really fundamental problems that we can still fundamental problems that we can still identify in academia that it doesn't matter how many chips you have, you can make a lot of progress, you know.

Um make a lot of progress, you know. Um first of all, interdisiplinary AI to me is a really exciting area in academia, especially for scientific academia, especially for scientific discovery.

There's just so many discovery. There's just so many disciplines that can cross AI.

I think disciplines that can cross AI. I think that's a big area uh that one could go to.

On the theoretical side, I find it fascinating that the AI capability has a fascinating that the AI capability has a 100% outrun theory. We don't know how, you know, we don't have explanability.

We don't know how to figure out the causality. There there's just so much in u in the the models we don't u in the the models we don't understand that one could push forward.

And um you know the list can go on in computer vision there's still computer vision there's still representational problems we haven't representational problems we haven't solved and also you know um small data that's another really interesting um domain and so yeah these are the possibilities. Thank you so much Fi.

Thank you professor Lee and congratulation again on your honorary doctorate from Yale. I on your honorary doctorate from Yale.

I was honored there to witness that moment was honored there to witness that moment one months ago and my question is in uh one months ago and my question is in uh in your perspective will AGI emerge more likely as a unified uh single unified model or as a multi- aent system. The way you ask this question is already two kind of definition.

One definition is kind of definition. One definition is more theoretical which is define AGI as if there is a IQ test that one passes that defines AGI.

The other part of your the other half of your question is much more utilitarian. Is it functional?

If it's agentbased, what tasks can it do? I struggle with this definition of AGI to be honest.

Here's why. the founding fathers of AI who came together in 1956 in Dartmouth you know the John McCarthy and Marvin Mins Minsky of them they wanted to solve the problem of machines that can think and that's a problem that can think and that's a problem that touring Alan Touring also put forward a touring Alan Touring also put forward a few years earlier 10 years or whatever earlier than them and that statement is not a narrow it's not a narrow AI.

It's a it it's a statement of intelligence. So I don't statement of intelligence.

So I don't really know how to differentiate that really know how to differentiate that founding question of AI versus this new word AGI. To me they're the same thing.

But I get it that the industry thing. But I get it that the industry today like to call AGI as if that's beyond AI.

And I struggle with that because I feel there I don't know what exactly is AGI different from AI. If we say today's AGIish system performs say today's AGIish system performs better than the narrower AI system in ' 80s, '7s, 90s or whatever, I think that's right.

That that's just the that's right. That that's just the progression of the field.

But progression of the field. But fundamentally, I think the science of AI is the science of intelligence is to create machines that can think and do to create machines that can think and do things as intelligently or even more things as intelligently or even more intelligently as humans.

So I don't know how to define AGI. So I don't know without defining it I don't know if it's monolithic.

If you look at the brain it's one thing you know you can call it monolithic but it does have different functionalities and you can even there's functionalities and you can even there's broco area for language. There's cort broco area for language.

There's cort visual cortex there's motor cortex. So visual cortex there's motor cortex.

So so I don't really know how to answer that question. Hi um my name is Yashna and I just want to say thank you.

I and I just want to say thank you. I think it's really inspiring to see a think it's really inspiring to see a woman playing a leading role in this and um as a researcher educator and entrepreneur I wanted to ask what type entrepreneur I wanted to ask what type of person do you think should pursue of person do you think should pursue graduate school in this rapid rise of AI?

That's a great question and that's a question even parents ask me. I really think graduate school is the four or think graduate school is the four or five years where you have burning curiosity.

You're led by curiosity and that curiosity is so strong that there's no better other place to do it. It's better other place to do it.

It's different from a startup because startup different from a startup because startup is not just it you have to be a little careful. Startup cannot be just led by curiosity.

Your investors will be mad at you. Um it's a startup has a more focused commercial goal and some part of focused commercial goal and some part of it is curiosity but it's not just it is curiosity but it's not just curiosity.

Whereas for grass group that curiosity. Whereas for grass group that curiosity to solve problem or to ask the right questions is so important that I think those going in with that intense curiosity would really enjoy the four or five would really enjoy the four or five years even if the outside world is years even if the outside world is passing by at the speed of light you'll passing by at the speed of light you'll still be happy because you're there still be happy because you're there following that curiosity.

I first I following that curiosity. I first I wanted to say thank you for your time to thank you for coming out to speak to us.

You mentioned that open sourcing was a You mentioned that open sourcing was a big part of the growth from imageet and now with the recent release and growth of large language models we've seen organizations taking different approaches with open source which with some organizations staying fully closed source some organizations fully releasing their entire research stack some being somewhere in the middle open some being somewhere in the middle open sourcing weights or having restrictive sourcing weights or having restrictive licenses and things of that nature.

So I wanted to ask what do you think of these different approaches to open source and what do you believe the right way to go about open source as an AI company is? I think the ecosystem is healthy when there are different approaches.

I'm not there are different approaches. I'm not religious in terms of you must open religious in terms of you must open source or you must close source.

It source or you must close source. It depends on the company's uh business depends on the company's uh business strategy.

And uh for example, it's clear strategy. And uh for example, it's clear why Facebook uh Meta wants to open why Facebook uh Meta wants to open source, right?

They they um they are source, right? They they um they are right now their business model is not selling the selling the model yet.

They they're using it to grow the ecosystem so that people come to their platform. So open source makes a lot of sense.

Whereas another company that is really monetizing on the even really monetizing on the even monetizing you can think about an open source tier and a closed source tier. So I'm pretty I'm pretty open to that I'm pretty I'm pretty open to that category or a meta level is I think open source is should be protected.

I think if there is efforts of open source both in public sector as like academia as well as uh private sector is so important. It's it's so important for the entrepreneurial ecosystem.

It's so important for public sector that I think that should be tech uh protected. It should shouldn't be uh penalized.

I have a question about data. So you have a question about data.

So you called very well the shift in machine called very well the shift in machine learning towards datadriven methods with learning towards datadriven methods with imageet. Now that you're working on imageet.

Now that you're working on world models uh and you mentioned that we don't have this spatial data on the internet, it exists only in our heads. internet, it exists only in our heads.

How are you solving this problem? What are you betting on?

Are you collecting this data from the real world? Are you doing synthetic data?

Do you believe in that or do you believe in good uh old priors? Thanks.

You should join World Labs and I'll tell you. Oh, it's a good Labs and I'll tell you.

Oh, it's a good one. Um look um as a company I'm not one.

Um look um as a company I'm not going to be able to share a lot but I think it's important to acknowledge that um we're taking a hybrid approach. It is really important to have a lot of data but also have a lot of quality data at the end of the day there is still garbage in garbage out if you're not careful with the quality of data.

So, we'll do one last question. Um, hi So, we'll do one last question.

Um, my name is Annie and thank you very much for speaking with us. Um, you very much for speaking with us.

Um, so in your book, The World I See, you talk the challenges you face as a immigrant girl and woman in STEM. Um, immigrant girl and woman in STEM.

Um, I'm curious to know if there's a time I'm curious to know if there's a time that you feel the moment of being a that you feel the moment of being a minority in the workplace and um, if so, minority in the workplace and um, if so, how did you manage to overcome this or persuade others? Thank you for that question.

I want to be very careful or thoughtful in answering you because we all come from different background we all come from different background and how each of us feel is is very and how each of us feel is is very unique. You know it almost doesn't unique.

You know it almost doesn't even matter what are the big categories. even matter what are the big categories.

All of us have moments that we feel were the minority or the only person in the room. So of course I felt that way.

Sometimes it's based on who I am. Sometimes it's based on my idea.

Sometimes it's just based on I don't know the the color of my shirt, whatever that is. Um I have But this whatever that is.

Um I have But this where I do want to encourage everybody. where I do want to encourage everybody.

Maybe it is because since I was young coming to this country, I kind of have experienced it is what it is. I am an immigrant woman.

I almost developed a capability to not overindex on that. I'm capability to not overindex on that.

I'm here just like every one of you. I'm here to learn or to do things or to create things.

That was a great answer. And I really all of you, you're about to embark on something or in the middle of embarking something and you're going to have moments of weakness or strangeness or I feel this every day, especially startup life.

Sometimes I'm like, "Oh my god, I don't know what I'm doing." Just focus on doing it. Gradient desend yourself to the optimized desend yourself to the optimized solution.

## Enhanced Quality Analysis

### Core Metrics
- **Total Lines:** 1857 → 921 (after deduplication)
- **Deduplication Effectiveness:** 50.4%
- **Quality Score:** 100.0%
- **Quality Rating:** Excellent
- **Safety Penalties:** -0 points

### Content Quality
- **Total Words:** 12,844
- **Average Words per Line:** 13.9
- **Average Line Length:** 81.4 characters
- **Has Punctuation:** Yes ✅
- **Timestamp Coverage:** 100.0%

### Duplicate Analysis
- **Exact Duplicates:** 0
- **Partial Duplicates:** 0

### ✅ Quality Assessment

No quality concerns detected.

### 📋 Recommendations

- ✅ Good quality transcript - suitable for analysis
- 🎯 Ready for AI processing and content analysis



## MCP Resource Usage

This transcript can be used as an MCP resource:

### Resource URI
```
transcript://_PioN-CpOP0
```

### Programmatic Access
```python
# In your MCP server
async def get_transcript(video_id: str):
    return await load_transcript_resource(video_id)
```

### Use Cases
- **Content Analysis:** Analyze themes, topics, and sentiment
- **Quote Extraction:** Find specific quotes or statements  
- **Study Notes:** Generate structured educational notes
- **Search & Discovery:** Full-text search within video content
- **Summarization:** Create abstracts and key points
- **Fact Checking:** Verify claims and statements

---

*Generated by YouTube to MCP Resource Tool v1.0*  
*For more information: https://github.com/your-repo/mcp-youtube-transcript*
